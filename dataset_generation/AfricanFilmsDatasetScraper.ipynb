{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-02T09:50:35.402752Z",
     "iopub.status.busy": "2025-01-02T09:50:35.402330Z",
     "iopub.status.idle": "2025-01-02T09:50:35.421367Z",
     "shell.execute_reply": "2025-01-02T09:50:35.420471Z",
     "shell.execute_reply.started": "2025-01-02T09:50:35.402719Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/generated/african_dataset.csv\n",
      "/kaggle/input/tmdb-data/TMDB_movie_dataset_v11.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T09:50:38.782988Z",
     "iopub.status.busy": "2025-01-02T09:50:38.782533Z",
     "iopub.status.idle": "2025-01-02T09:50:49.621477Z",
     "shell.execute_reply": "2025-01-02T09:50:49.620156Z",
     "shell.execute_reply.started": "2025-01-02T09:50:38.782951Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wikipedia-api\n",
      "  Downloading wikipedia_api-0.7.1.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.5)\n",
      "Collecting unidecode\n",
      "  Downloading Unidecode-1.3.8-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting wikipedia\n",
      "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from wikipedia-api) (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (4.12.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (2024.8.30)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia) (2.6)\n",
      "Downloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: wikipedia-api, wikipedia\n",
      "  Building wheel for wikipedia-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for wikipedia-api: filename=Wikipedia_API-0.7.1-py3-none-any.whl size=14347 sha256=824e9ab98b36048506774a31f8b88d2e18adc6ef1298cb279165f84328ea9ef0\n",
      "  Stored in directory: /root/.cache/pip/wheels/4c/96/18/b9201cc3e8b47b02b510460210cfd832ccf10c0c4dd0522962\n",
      "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11679 sha256=cd93306c0a1d3ecf098f83f6d6555bee98b41bf577dddce5f5e7316e6c726f5e\n",
      "  Stored in directory: /root/.cache/pip/wheels/5e/b6/c5/93f3dec388ae76edc830cb42901bb0232504dfc0df02fc50de\n",
      "Successfully built wikipedia-api wikipedia\n",
      "Installing collected packages: unidecode, wikipedia-api, wikipedia\n",
      "Successfully installed unidecode-1.3.8 wikipedia-1.4.0 wikipedia-api-0.7.1\n"
     ]
    }
   ],
   "source": [
    "!pip install wikipedia-api tqdm unidecode wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T09:50:53.432380Z",
     "iopub.status.busy": "2025-01-02T09:50:53.431850Z",
     "iopub.status.idle": "2025-01-02T09:50:53.625061Z",
     "shell.execute_reply": "2025-01-02T09:50:53.623721Z",
     "shell.execute_reply.started": "2025-01-02T09:50:53.432334Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import wikipediaapi\n",
    "import time\n",
    "import re\n",
    "from urllib.parse import unquote\n",
    "from tqdm import tqdm\n",
    "import unidecode\n",
    "from datetime import datetime\n",
    "import concurrent.futures\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import wikipedia\n",
    "import wikipedia.wikipedia\n",
    "wikipedia.wikipedia.BeautifulSoup = lambda html: BeautifulSoup(html, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T21:39:54.984566Z",
     "iopub.status.busy": "2025-01-01T21:39:54.984201Z",
     "iopub.status.idle": "2025-01-01T21:39:54.989245Z",
     "shell.execute_reply": "2025-01-01T21:39:54.988183Z",
     "shell.execute_reply.started": "2025-01-01T21:39:54.984532Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def setup_wiki():\n",
    "    \"\"\"Setup Wikipedia API with a custom user agent\"\"\"\n",
    "    user_agent = \"AfricanMoviesDataset/1.0 (nathan.mbugua@strathmore.edu)\"\n",
    "    wiki = wikipediaapi.Wikipedia(\n",
    "        language='en',\n",
    "        extract_format=wikipediaapi.ExtractFormat.WIKI,\n",
    "        user_agent=user_agent\n",
    "    )\n",
    "    return wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T21:39:56.903753Z",
     "iopub.status.busy": "2025-01-01T21:39:56.903365Z",
     "iopub.status.idle": "2025-01-01T21:39:56.908882Z",
     "shell.execute_reply": "2025-01-01T21:39:56.907825Z",
     "shell.execute_reply.started": "2025-01-01T21:39:56.903704Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def fetch_page_content(url):\n",
    "    \"\"\"Fetch page content with error handling\"\"\"\n",
    "    headers = {\"User-Agent\": \"AfricanMoviesDataset/1.0 (nathan.mbugua@strathmore.edu)\"}\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        return BeautifulSoup(response.content, 'html.parser')\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Failed to fetch URL: {url} (Error: {e})\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T21:40:25.104599Z",
     "iopub.status.busy": "2025-01-01T21:40:25.104238Z",
     "iopub.status.idle": "2025-01-01T21:40:25.112645Z",
     "shell.execute_reply": "2025-01-01T21:40:25.111615Z",
     "shell.execute_reply.started": "2025-01-01T21:40:25.104565Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_movie_plot(wiki, movie_url):\n",
    "    \"\"\"Extract plot from movie page\"\"\"\n",
    "    if not movie_url:\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        page_title = movie_url.split(\"/wiki/\")[-1]\n",
    "        page_title = unquote(page_title)\n",
    "        page = wiki.page(page_title)\n",
    "        if page.exists():\n",
    "            plot_sections = ['Plot', 'Subject', 'Synopsis', 'Description', 'Story']\n",
    "            for section in page.sections:\n",
    "                if section.title in plot_sections:\n",
    "                    return section.text\n",
    "            \n",
    "            # Fallback: If ano plot section found and page is short, use content available\n",
    "            if len(page.text) < 5000:\n",
    "                # Get the raw HTML content of the movie page using requests\n",
    "                url = f\"https://en.wikipedia.org/wiki/{page_title}\"\n",
    "                response = requests.get(url)\n",
    "\n",
    "                if response.status_code == 200:\n",
    "                    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "                    content_div = soup.find('div', {'id': 'mw-content-text'})\n",
    "            \n",
    "                    if content_div:\n",
    "                        # Find all <p> tags within the div\n",
    "                        p_tags = content_div.find_all('p')\n",
    "                \n",
    "                        # If there are at least two <p> tags, use the text from them\n",
    "                        if len(p_tags) >= 2:\n",
    "                            plot_text = p_tags[0].get_text(strip=True) + \" \" + p_tags[1].get_text(strip=True)\n",
    "                            return plot_text\n",
    "                        # If only one <p> tag is found, return its text\n",
    "                        elif len(p_tags) > 0:\n",
    "                            return p_tags[0].get_text(strip=True)\n",
    "                    \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching plot for {movie_url}: {e}\")\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T21:40:30.136416Z",
     "iopub.status.busy": "2025-01-01T21:40:30.136033Z",
     "iopub.status.idle": "2025-01-01T21:40:30.142363Z",
     "shell.execute_reply": "2025-01-01T21:40:30.141026Z",
     "shell.execute_reply.started": "2025-01-01T21:40:30.136385Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def extract_release_date(text, current_date=None):\n",
    "    \"\"\"Extract release date from text with various formats\"\"\"\n",
    "    # Try year in parentheses\n",
    "    year_match = re.search(r'\\((\\d{4})\\)', text)\n",
    "    if year_match:\n",
    "        return year_match.group(1)\n",
    "    \n",
    "    # Try isolated 4-digit year\n",
    "    year_match = re.search(r'\\b(\\d{4})\\b', text)\n",
    "    if year_match:\n",
    "        return year_match.group(1)\n",
    "    \n",
    "    return current_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T21:40:34.236561Z",
     "iopub.status.busy": "2025-01-01T21:40:34.236248Z",
     "iopub.status.idle": "2025-01-01T21:40:34.242509Z",
     "shell.execute_reply": "2025-01-01T21:40:34.241273Z",
     "shell.execute_reply.started": "2025-01-01T21:40:34.236536Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def clean_page_content(soup):\n",
    "    \"\"\"Remove unwanted sections and tables from the page\"\"\"\n",
    "    if not soup:\n",
    "        return soup\n",
    "        \n",
    "    # Remove external links sections\n",
    "    for table_class in ['nowraplinks', 'sidebar', 'navigation']:\n",
    "        external_sections = soup.find_all('table', {'class': table_class})\n",
    "        for section in external_sections:\n",
    "            section.extract()\n",
    "            \n",
    "    # Remove navigation boxes\n",
    "    nav_boxes = soup.find_all('div', {'class': ['navbox', 'vertical-navbox']})\n",
    "    for nav_box in nav_boxes:\n",
    "        nav_box.extract()\n",
    "        \n",
    "    # Remove reference sections\n",
    "    ref_sections = soup.find_all('div', {'class': 'reflist'})\n",
    "    for ref_section in ref_sections:\n",
    "        ref_section.extract()\n",
    "        \n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T21:40:44.259001Z",
     "iopub.status.busy": "2025-01-01T21:40:44.258565Z",
     "iopub.status.idle": "2025-01-01T21:40:44.269177Z",
     "shell.execute_reply": "2025-01-01T21:40:44.267893Z",
     "shell.execute_reply.started": "2025-01-01T21:40:44.258971Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def process_table_movies(table, country_url, use_url_for_year=False):\n",
    "    \"\"\"Process movies from a table structure\"\"\"\n",
    "    movies = []\n",
    "    if not table:\n",
    "        return movies\n",
    "\n",
    "    # Clean the table content first\n",
    "    table = clean_page_content(BeautifulSoup(str(table), 'html.parser'))\n",
    "    if not table:\n",
    "        return movies\n",
    "\n",
    "    rows = table.find_all('tr')\n",
    "    if not rows:\n",
    "        return movies\n",
    "\n",
    "    # Find title and release date columns\n",
    "    headers = rows[0].find_all('th')\n",
    "    title_idx = None\n",
    "    date_idx = None\n",
    "    \n",
    "    for idx, header in enumerate(headers):\n",
    "        header_text = header.get_text().lower()\n",
    "        if (\"title\" in header_text or \"film\" in header_text) and \"film genre\" not in header_text:\n",
    "            title_idx = idx\n",
    "        if any(date_term in header_text for date_term in [\"year\", \"date\", \"release\"]):\n",
    "            date_idx = idx\n",
    "\n",
    "    if title_idx is None:\n",
    "        print(f\"No title column found in {country_url}\")\n",
    "        return movies\n",
    "\n",
    "    # Extract year from the country_url if needed\n",
    "    year_from_url = None\n",
    "    if use_url_for_year:\n",
    "        match = re.search(r'(\\d{4})$', country_url)\n",
    "        if match:\n",
    "            year_from_url = match.group(1)\n",
    "\n",
    "    current_date = None\n",
    "    for row in rows[1:]:\n",
    "        cols = row.find_all(['td', 'th'])\n",
    "\n",
    "        # Check for year row\n",
    "        if len(cols) == 1 and 'colspan' in cols[0].attrs:\n",
    "            current_date = cols[0].get_text(strip=True)\n",
    "            continue\n",
    "\n",
    "        if len(cols) > title_idx:\n",
    "            title_col = cols[title_idx]\n",
    "            link = title_col.find('a')\n",
    "            \n",
    "            if link and link.get('href') and \"redlink=1\" not in link['href']:\n",
    "                movie_url = \"https://en.wikipedia.org\" + link['href']\n",
    "                release_date = None\n",
    "                \n",
    "                if date_idx is not None and len(cols) > date_idx:\n",
    "                    release_date = cols[date_idx].get_text(strip=True)\n",
    "                if not release_date:\n",
    "                    release_date = current_date\n",
    "                if not release_date and year_from_url:\n",
    "                    release_date = year_from_url\n",
    "\n",
    "                movies.append({\n",
    "                    'Title': re.sub(r'\\[.*?\\]', '', title_col.get_text(strip=True)).strip(),\n",
    "                    'Release Date': release_date,\n",
    "                    'URL': movie_url\n",
    "                })\n",
    "\n",
    "    return movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T22:16:27.175011Z",
     "iopub.status.busy": "2025-01-01T22:16:27.174557Z",
     "iopub.status.idle": "2025-01-01T22:16:27.182888Z",
     "shell.execute_reply": "2025-01-01T22:16:27.181802Z",
     "shell.execute_reply.started": "2025-01-01T22:16:27.174977Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def process_list_movies(content_section, in_main_page=False):\n",
    "    \"\"\"Process movies from a list structure\"\"\"\n",
    "    movies = []\n",
    "    if not content_section:  # Add this check\n",
    "        return movies\n",
    "\n",
    "    # Clean the content section first\n",
    "    content_section = clean_page_content(BeautifulSoup(str(content_section), 'html.parser'))\n",
    "    if not content_section:\n",
    "        return movies\n",
    "        \n",
    "    movie_items = []\n",
    "\n",
    "    for ul in content_section.find_all('ul'):\n",
    "        # Check if the previous h2 is not one of the excluded sections\n",
    "        prev_h2 = ul.find_previous('h2')\n",
    "        if not prev_h2 or prev_h2.get('id') not in ['See_also', 'References', 'External_links']:\n",
    "            movie_items.extend(ul.find_all('li'))\n",
    "    \n",
    "    for item in movie_items:\n",
    "        link = item.find('a')\n",
    "        if link and link.get('href'):\n",
    "            href = link.get('href')\n",
    "            if \"redlink=1\" not in href and href.startswith('/wiki/'):\n",
    "                movie_url = \"https://en.wikipedia.org\" + href\n",
    "                title = link.get_text(strip=True)\n",
    "                text_after_link = item.get_text(strip=True).replace(title, \"\", 1).strip()\n",
    "\n",
    "                release_date = extract_release_date(text_after_link)\n",
    "                director = None\n",
    "\n",
    "                # Extract director if present\n",
    "                if \"by\" in text_after_link:\n",
    "                    director_match = re.search(r'by\\s+([^(]+)', text_after_link)\n",
    "                    if director_match:\n",
    "                        director = director_match.group(1).strip()\n",
    "\n",
    "                movie_data = {\n",
    "                    'Title': title,\n",
    "                    'Release Date': release_date,\n",
    "                    'URL': movie_url,\n",
    "                    'Director': director\n",
    "                }\n",
    "                if in_main_page:\n",
    "                    wiki = setup_wiki()\n",
    "                    movie_data['Plot'] = get_movie_plot(wiki, movie_url)\n",
    "                \n",
    "                movies.append(movie_data)\n",
    "    \n",
    "    return movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T22:27:19.706714Z",
     "iopub.status.busy": "2025-01-01T22:27:19.706353Z",
     "iopub.status.idle": "2025-01-01T22:27:19.713355Z",
     "shell.execute_reply": "2025-01-01T22:27:19.712269Z",
     "shell.execute_reply.started": "2025-01-01T22:27:19.706679Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def process_nigerian_style_page(soup):\n",
    "    \"\"\"Process Nigerian-style pages with year links\"\"\"\n",
    "    movies = []\n",
    "    if not soup:\n",
    "        return movies\n",
    "        \n",
    "    # Clean the page content first\n",
    "    soup = clean_page_content(soup)\n",
    "\n",
    "    # Find year links in the sidebar or content\n",
    "    year_patterns = [\n",
    "        r'List_of_Nigerian_films_of_\\d{4}',\n",
    "        r'List_of_Nigerian_films_of_the_\\d{4}s'\n",
    "    ]\n",
    "    \n",
    "    for pattern in year_patterns:\n",
    "        year_links = soup.find_all('a', href=re.compile(pattern))\n",
    "        for link in year_links:\n",
    "            if 'redlink=1' not in link.get('href', ''):\n",
    "                year_url = f\"https://en.wikipedia.org{link['href']}\"\n",
    "                year_soup = fetch_page_content(year_url)\n",
    "                if year_soup:\n",
    "                    # Clean the year page content\n",
    "                    year_soup = clean_page_content(year_soup)\n",
    "                    # Process movies from the year page\n",
    "                    year_movies = process_table_movies(\n",
    "                        year_soup.find('table', {'class': 'wikitable'}),\n",
    "                        year_url,\n",
    "                        use_url_for_year=True\n",
    "                    )\n",
    "                    movies.extend(year_movies)\n",
    "                time.sleep(0.2)\n",
    "\n",
    "    return movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T22:27:21.832645Z",
     "iopub.status.busy": "2025-01-01T22:27:21.832293Z",
     "iopub.status.idle": "2025-01-01T22:27:21.839432Z",
     "shell.execute_reply": "2025-01-01T22:27:21.838234Z",
     "shell.execute_reply.started": "2025-01-01T22:27:21.832612Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def process_egyptian_style_page(soup):\n",
    "    \"\"\"Process Egyptian-style pages with multiple 'hlist' divs containing year links.\"\"\"\n",
    "    movies = []\n",
    "    if not soup:\n",
    "        return movies\n",
    "\n",
    "    # Clean the page content first\n",
    "    soup = clean_page_content(soup)\n",
    "\n",
    "    # Find all \"hlist\" divs containing the year links\n",
    "    hlist_divs = soup.find_all('div', {'class': 'hlist'})\n",
    "    if not hlist_divs:\n",
    "        return movies\n",
    "\n",
    "    # Loop through each hlist div and extract year links\n",
    "    for hlist_div in hlist_divs:\n",
    "        year_links = hlist_div.find_all('a', href=True)\n",
    "        for link in year_links:\n",
    "            # Skip redlinks by checking if 'redlink=1' is in the href\n",
    "            if 'redlink=1' in link['href']:\n",
    "                continue  # Skip processing for redlink\n",
    "            \n",
    "            year_url = f\"https://en.wikipedia.org{link['href']}\"\n",
    "            year_soup = fetch_page_content(year_url)\n",
    "            if year_soup:\n",
    "                # Clean the year page content\n",
    "                year_soup = clean_page_content(year_soup)\n",
    "                # Process movies from the year page\n",
    "                year_movies = process_table_movies(\n",
    "                    year_soup.find('table', {'class': 'wikitable'}),\n",
    "                    year_url,\n",
    "                    use_url_for_year=True\n",
    "                )\n",
    "                movies.extend(year_movies)\n",
    "            time.sleep(0.2)  # Add delay to prevent being blocked by Wikipedia\n",
    "\n",
    "    return movies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T22:25:15.171182Z",
     "iopub.status.busy": "2025-01-01T22:25:15.170844Z",
     "iopub.status.idle": "2025-01-01T22:25:15.181899Z",
     "shell.execute_reply": "2025-01-01T22:25:15.180765Z",
     "shell.execute_reply.started": "2025-01-01T22:25:15.171155Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def process_main_page_movies(soup):\n",
    "    \"\"\"Process movies listed directly on the main page under country sections.\"\"\"\n",
    "    all_movies = []\n",
    "    wiki = setup_wiki()\n",
    "    \n",
    "    # Find all sections containing country headers\n",
    "    sections = soup.find_all('div', {'class': 'mw-heading'})\n",
    "    \n",
    "    # Filter out unwanted sections\n",
    "    filtered_sections = [\n",
    "        section for section in sections\n",
    "        if section.find('h2') and section.find('h2').get('id') not in ['See_also', 'External_links', 'References']\n",
    "    ]\n",
    "    \n",
    "    for section in filtered_sections:\n",
    "        # Get country header\n",
    "        header = section.find('h2')\n",
    "        if not header:\n",
    "            continue\n",
    "            \n",
    "        country_link = header.find('a')\n",
    "        if not country_link:\n",
    "            continue\n",
    "            \n",
    "        country = country_link.text.strip()\n",
    "        if not country:\n",
    "            continue\n",
    "                    \n",
    "        # Find the next sibling for either a table or a list\n",
    "        next_section = section.find_next_sibling()\n",
    "        if next_section:\n",
    "            # Check for tables\n",
    "            if next_section.name == 'table' and 'wikitable' in next_section.get('class', []):\n",
    "                table = next_section\n",
    "                movies = []\n",
    "                rows = table.find_all('tr')[1:]  # Skip header row\n",
    "                \n",
    "                for row in rows:\n",
    "                    cols = row.find_all(['td', 'th'])\n",
    "                    if len(cols) >= 2:  # Need at least year and title\n",
    "                        year = cols[0].text.strip()\n",
    "                        title_col = cols[1]\n",
    "                        director = cols[2].text.strip() if len(cols) > 2 else None\n",
    "                        \n",
    "                        link = title_col.find('a')\n",
    "                        if link and link.get('href') and \"redlink=1\" not in link['href']:\n",
    "                            movie_url = \"https://en.wikipedia.org\" + link['href']\n",
    "                            title = title_col.text.strip()\n",
    "                            print(f\"Processing: {title} ({year})\")\n",
    "                            \n",
    "                            movies.append({\n",
    "                                'Title': title,\n",
    "                                'Release Date': year,\n",
    "                                'URL': movie_url,\n",
    "                                'Plot': get_movie_plot(wiki, movie_url),\n",
    "                                'Director': director,\n",
    "                                'Country': country\n",
    "                            })\n",
    "                all_movies.extend(movies)\n",
    "            \n",
    "            # Check for lists\n",
    "            elif next_section.name == 'ul':\n",
    "                list_movies = process_list_movies(next_section, in_main_page=True)\n",
    "                for movie in list_movies:\n",
    "                    movie['Country'] = country\n",
    "                all_movies.extend(list_movies)\n",
    "    \n",
    "    return all_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T22:27:27.324253Z",
     "iopub.status.busy": "2025-01-01T22:27:27.323902Z",
     "iopub.status.idle": "2025-01-01T22:27:27.335175Z",
     "shell.execute_reply": "2025-01-01T22:27:27.333986Z",
     "shell.execute_reply.started": "2025-01-01T22:27:27.324221Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_african_movies_dataset():\n",
    "    \"\"\"Main function to create the dataset\"\"\"\n",
    "    wiki = setup_wiki()\n",
    "    base_url = \"https://en.wikipedia.org/wiki/List_of_African_films\"\n",
    "    all_movies = []\n",
    "    \n",
    "    # Get main page\n",
    "    soup = fetch_page_content(base_url)\n",
    "    if not soup:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    soup = clean_page_content(soup)\n",
    "\n",
    "    # Process all country links\n",
    "    content_section = soup.find('div', {'id': 'mw-content-text'})\n",
    "    if not content_section:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Process movies listed directly on the main page\n",
    "    main_page_movies = process_main_page_movies(soup)\n",
    "    all_movies.extend(main_page_movies)\n",
    "\n",
    "    # Modified this part to better filter country links\n",
    "    country_links = []\n",
    "    for link in content_section.find_all('a', href=re.compile(r'List_of_.*_films')):\n",
    "        href = link.get('href', '')\n",
    "        # Skip edit links, redlinks, and specific unwanted links\n",
    "        if ('redlink=1' not in href and \n",
    "            'action=edit' not in href and \n",
    "            link.text.strip() != 'edit' and \n",
    "            not href.startswith('#See_also') and \n",
    "            not href.startswith('#References') and \n",
    "            not href.startswith('#External_links')):\n",
    "            country_links.append(link)\n",
    "    \n",
    "    for link in country_links:\n",
    "        href = link.get('href', '')\n",
    "        country = link.text.strip()\n",
    "        \n",
    "        # Skip if country is empty or just \"edit\"\n",
    "        if not country or country.lower() == 'edit':\n",
    "            continue\n",
    "            \n",
    "        print(f\"Processing country: {country}\")\n",
    "        \n",
    "        # Movies are in separate page\n",
    "        country_url = f\"https://en.wikipedia.org{href}\"\n",
    "        print(country_url)\n",
    "        country_soup = fetch_page_content(country_url)\n",
    "            \n",
    "        if country_soup:\n",
    "            if 'Nigerian' in country_url:\n",
    "                movies = process_nigerian_style_page(country_soup)\n",
    "            elif 'Egyptian' in country_url:\n",
    "                movies = process_egyptian_style_page(country_soup)\n",
    "            elif 'Burkinab' in country_url:\n",
    "                table_movies = process_table_movies(\n",
    "                    country_soup.find_all('table', {'class': 'wikitable'}),\n",
    "                    country_url\n",
    "                )\n",
    "                movies = table_movies\n",
    "            else:\n",
    "                table_movies = process_table_movies(\n",
    "                    country_soup.find_all('table', {'class': 'wikitable'}),\n",
    "                    country_url\n",
    "                )\n",
    "                list_movies = process_list_movies(\n",
    "                    country_soup.find('div', {'class': 'mw-parser-output'})\n",
    "                )\n",
    "                movies = table_movies + list_movies\n",
    "\n",
    "            # Add country to movies from separate pages\n",
    "            for movie in movies:\n",
    "                movie['Country'] = country\n",
    "\n",
    "            # Get plots for movies\n",
    "            for movie in movies:\n",
    "                print(f\"Processing: {movie['Title']} ({movie['Release Date'] or 'Unknown date'})\")\n",
    "                plot = get_movie_plot(wiki, movie['URL'])\n",
    "                movie['Plot'] = plot\n",
    "                all_movies.append(movie)\n",
    "                time.sleep(0.2)\n",
    "\n",
    "    df = pd.DataFrame(all_movies)\n",
    "    print(\"Dataset created successfully\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T22:27:30.787975Z",
     "iopub.status.busy": "2025-01-01T22:27:30.787576Z",
     "iopub.status.idle": "2025-01-01T22:36:21.356127Z",
     "shell.execute_reply": "2025-01-01T22:36:21.355128Z",
     "shell.execute_reply.started": "2025-01-01T22:27:30.787945Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "all_dfs = create_african_movies_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-01-01T22:38:29.002171Z",
     "iopub.status.busy": "2025-01-01T22:38:29.001831Z",
     "iopub.status.idle": "2025-01-01T22:38:29.014121Z",
     "shell.execute_reply": "2025-01-01T22:38:29.013081Z",
     "shell.execute_reply.started": "2025-01-01T22:38:29.002145Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Release Date</th>\n",
       "      <th>URL</th>\n",
       "      <th>Plot</th>\n",
       "      <th>Director</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Gods Must Be Crazy</td>\n",
       "      <td>1980</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Gods_Must_Be...</td>\n",
       "      <td>Xi and his San tribe live happily in the Kalah...</td>\n",
       "      <td>Jamie Uys</td>\n",
       "      <td>Botswana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Gods Must Be Crazy II</td>\n",
       "      <td>1989</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Gods_Must_Be...</td>\n",
       "      <td>The film has four storylines, which run in par...</td>\n",
       "      <td>Jamie Uys</td>\n",
       "      <td>Botswana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gito l'ingrat</td>\n",
       "      <td>1992</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Gito_l%27ingrat</td>\n",
       "      <td>Gito is a Burundian student who lives in Paris...</td>\n",
       "      <td>Léonce Ngabo</td>\n",
       "      <td>Burundi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Amílcar Cabral</td>\n",
       "      <td>2000</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Am%C3%ADlcar_Cab...</td>\n",
       "      <td>Amílcar Cabralis adocumentary filmdirected byA...</td>\n",
       "      <td>Ana Ramos Lisoba</td>\n",
       "      <td>Cape Verde</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hanami</td>\n",
       "      <td>2024</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Hanami_(film)</td>\n",
       "      <td>Hanamiis a drama film, directed by Denise Fern...</td>\n",
       "      <td>Denise Fernandes</td>\n",
       "      <td>Cape Verde</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>White Hotel</td>\n",
       "      <td>2001</td>\n",
       "      <td>https://en.wikipedia.org/wiki/White_Hotel_(film)</td>\n",
       "      <td>When two women with a video camera follow an A...</td>\n",
       "      <td>Dianne Griffin, Tobi Solvang</td>\n",
       "      <td>Eritrea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Welcome to the Smiling Coast: Living in the Ga...</td>\n",
       "      <td>None</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Welcome_to_the_S...</td>\n",
       "      <td>Welcome to the Smiling Coast: Living in the Ga...</td>\n",
       "      <td>None</td>\n",
       "      <td>Gambia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Jaha's Promise</td>\n",
       "      <td>None</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Jaha%27s_Promise</td>\n",
       "      <td>Jaha's Promise, is a 2017American-Gambiandocum...</td>\n",
       "      <td>None</td>\n",
       "      <td>Gambia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The Boy Kumasenu</td>\n",
       "      <td>1952</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Boy_Kumasenu</td>\n",
       "      <td>The film tells the story of a boy called Kumas...</td>\n",
       "      <td>Sean Graham</td>\n",
       "      <td>Ghana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Love Brewed in the African Pot</td>\n",
       "      <td>1980</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Love_Brewed_in_t...</td>\n",
       "      <td>The film takes place in Ghana during the colon...</td>\n",
       "      <td>Kwaw Ansah</td>\n",
       "      <td>Ghana</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title Release Date  \\\n",
       "0                             The Gods Must Be Crazy         1980   \n",
       "1                          The Gods Must Be Crazy II         1989   \n",
       "2                                      Gito l'ingrat         1992   \n",
       "3                                     Amílcar Cabral         2000   \n",
       "4                                             Hanami         2024   \n",
       "5                                        White Hotel         2001   \n",
       "6  Welcome to the Smiling Coast: Living in the Ga...         None   \n",
       "7                                     Jaha's Promise         None   \n",
       "8                                   The Boy Kumasenu         1952   \n",
       "9                     Love Brewed in the African Pot         1980   \n",
       "\n",
       "                                                 URL  \\\n",
       "0  https://en.wikipedia.org/wiki/The_Gods_Must_Be...   \n",
       "1  https://en.wikipedia.org/wiki/The_Gods_Must_Be...   \n",
       "2      https://en.wikipedia.org/wiki/Gito_l%27ingrat   \n",
       "3  https://en.wikipedia.org/wiki/Am%C3%ADlcar_Cab...   \n",
       "4        https://en.wikipedia.org/wiki/Hanami_(film)   \n",
       "5   https://en.wikipedia.org/wiki/White_Hotel_(film)   \n",
       "6  https://en.wikipedia.org/wiki/Welcome_to_the_S...   \n",
       "7     https://en.wikipedia.org/wiki/Jaha%27s_Promise   \n",
       "8     https://en.wikipedia.org/wiki/The_Boy_Kumasenu   \n",
       "9  https://en.wikipedia.org/wiki/Love_Brewed_in_t...   \n",
       "\n",
       "                                                Plot  \\\n",
       "0  Xi and his San tribe live happily in the Kalah...   \n",
       "1  The film has four storylines, which run in par...   \n",
       "2  Gito is a Burundian student who lives in Paris...   \n",
       "3  Amílcar Cabralis adocumentary filmdirected byA...   \n",
       "4  Hanamiis a drama film, directed by Denise Fern...   \n",
       "5  When two women with a video camera follow an A...   \n",
       "6  Welcome to the Smiling Coast: Living in the Ga...   \n",
       "7  Jaha's Promise, is a 2017American-Gambiandocum...   \n",
       "8  The film tells the story of a boy called Kumas...   \n",
       "9  The film takes place in Ghana during the colon...   \n",
       "\n",
       "                       Director     Country  \n",
       "0                     Jamie Uys    Botswana  \n",
       "1                     Jamie Uys    Botswana  \n",
       "2                  Léonce Ngabo     Burundi  \n",
       "3              Ana Ramos Lisoba  Cape Verde  \n",
       "4              Denise Fernandes  Cape Verde  \n",
       "5  Dianne Griffin, Tobi Solvang     Eritrea  \n",
       "6                          None      Gambia  \n",
       "7                          None      Gambia  \n",
       "8                   Sean Graham       Ghana  \n",
       "9                    Kwaw Ansah       Ghana  "
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_dfs.shape\n",
    "all_dfs.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T00:01:44.207690Z",
     "iopub.status.busy": "2025-01-02T00:01:44.207311Z",
     "iopub.status.idle": "2025-01-02T00:01:44.240520Z",
     "shell.execute_reply": "2025-01-02T00:01:44.239461Z",
     "shell.execute_reply.started": "2025-01-02T00:01:44.207632Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "all_dfs.to_csv('../datasets/generated/african_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T09:53:48.116026Z",
     "iopub.status.busy": "2025-01-02T09:53:48.115450Z",
     "iopub.status.idle": "2025-01-02T09:54:08.296321Z",
     "shell.execute_reply": "2025-01-02T09:54:08.295246Z",
     "shell.execute_reply.started": "2025-01-02T09:53:48.115979Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1153446, 24)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmdb_df = pd.read_csv('../datasets/TMDB_movie_dataset_v11.csv')\n",
    "tmdb_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T01:19:14.921672Z",
     "iopub.status.busy": "2025-01-02T01:19:14.921333Z",
     "iopub.status.idle": "2025-01-02T01:19:14.977284Z",
     "shell.execute_reply": "2025-01-02T01:19:14.976288Z",
     "shell.execute_reply.started": "2025-01-02T01:19:14.921642Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(956, 6)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "african_df = pd.read_csv('../datasets/generated/african_dataset.csv')\n",
    "african_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T02:16:24.758121Z",
     "iopub.status.busy": "2025-01-02T02:16:24.757742Z",
     "iopub.status.idle": "2025-01-02T02:16:24.764175Z",
     "shell.execute_reply": "2025-01-02T02:16:24.762863Z",
     "shell.execute_reply.started": "2025-01-02T02:16:24.758085Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def extract_title_variations(title):\n",
    "    \"\"\"\n",
    "    Extract different variations of a title, including those in parentheses.\n",
    "    \"\"\"\n",
    "    if pd.isna(title):\n",
    "        return []\n",
    "    \n",
    "    title = str(title).lower().strip()\n",
    "    variations = [title]\n",
    "    \n",
    "    # Extract main title (before first parenthesis)\n",
    "    if '(' in title:\n",
    "        main_title = title.split('(')[0].strip()\n",
    "        variations.append(main_title)\n",
    "        \n",
    "        # Extract all text within parentheses\n",
    "        parentheses_matches = re.findall(r'\\((.*?)\\)', title)\n",
    "        variations.extend([match.strip() for match in parentheses_matches])\n",
    "    \n",
    "    return variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T02:34:22.564497Z",
     "iopub.status.busy": "2025-01-02T02:34:22.564066Z",
     "iopub.status.idle": "2025-01-02T02:34:22.569885Z",
     "shell.execute_reply": "2025-01-02T02:34:22.568757Z",
     "shell.execute_reply.started": "2025-01-02T02:34:22.564465Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def clean_title(title):\n",
    "    \"\"\"\n",
    "    Clean and standardize movie titles.\n",
    "    \"\"\"\n",
    "    if pd.isna(title):\n",
    "        return ''\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    title = str(title).lower().strip()\n",
    "    \n",
    "    # Remove special characters and extra spaces\n",
    "    title = re.sub(r'[^\\w\\s()-]', '', title)\n",
    "    title = re.sub(r'\\s+', ' ', title)\n",
    "    \n",
    "    return title.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T02:34:38.180530Z",
     "iopub.status.busy": "2025-01-02T02:34:38.180131Z",
     "iopub.status.idle": "2025-01-02T02:34:38.186632Z",
     "shell.execute_reply": "2025-01-02T02:34:38.185490Z",
     "shell.execute_reply.started": "2025-01-02T02:34:38.180499Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def preprocess_dataframes(original_df, tmdb_df):\n",
    "    \"\"\"\n",
    "    Preprocess both dataframes by adding cleaned titles and extracting years.\n",
    "    \"\"\"\n",
    "    # Create copies to avoid modifying original data\n",
    "    original_df = original_df.copy()\n",
    "    tmdb_df = tmdb_df.copy()\n",
    "    \n",
    "    # Clean titles\n",
    "    original_df['clean_title'] = original_df['Title'].apply(clean_title)\n",
    "    tmdb_df['clean_title'] = tmdb_df['title'].apply(clean_title)\n",
    "    tmdb_df['clean_original_title'] = tmdb_df['original_title'].apply(clean_title)\n",
    "    \n",
    "    # Extract year from release_date in tmdb_df\n",
    "    tmdb_df['release_year'] = tmdb_df['release_date'].apply(\n",
    "        lambda x: int(x[:4]) if pd.notna(x) and len(x) >= 4 else None\n",
    "    )\n",
    "    \n",
    "    return original_df, tmdb_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T02:23:51.965621Z",
     "iopub.status.busy": "2025-01-02T02:23:51.965248Z",
     "iopub.status.idle": "2025-01-02T02:23:51.971200Z",
     "shell.execute_reply": "2025-01-02T02:23:51.970012Z",
     "shell.execute_reply.started": "2025-01-02T02:23:51.965588Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def years_match(year1, year2, tolerance=2):\n",
    "    \"\"\"\n",
    "    Check if two years are within a specified tolerance of each other\n",
    "    \"\"\"\n",
    "    if pd.isna(year1) or pd.isna(year2):\n",
    "        return True  # Accept matches without years\n",
    "    try:\n",
    "        return abs(int(year1) - int(year2)) <= tolerance\n",
    "    except (ValueError, TypeError):\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T02:35:46.272442Z",
     "iopub.status.busy": "2025-01-02T02:35:46.272064Z",
     "iopub.status.idle": "2025-01-02T02:35:46.278365Z",
     "shell.execute_reply": "2025-01-02T02:35:46.277331Z",
     "shell.execute_reply.started": "2025-01-02T02:35:46.272412Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def find_exact_match(title, year, tmdb_df):\n",
    "    \"\"\"\n",
    "    Find exact matches in either title or original_title.\n",
    "    \"\"\"\n",
    "    # Check clean_title\n",
    "    matches = tmdb_df[tmdb_df['clean_title'] == title]\n",
    "    if not matches.empty:\n",
    "        year_filtered = matches[matches['release_year'].apply(lambda x: years_match(x, year))]\n",
    "        if not year_filtered.empty:\n",
    "            return year_filtered.iloc[0]\n",
    "    \n",
    "    # Check clean_original_title\n",
    "    matches = tmdb_df[tmdb_df['clean_original_title'] == title]\n",
    "    if not matches.empty:\n",
    "        year_filtered = matches[matches['release_year'].apply(lambda x: years_match(x, year))]\n",
    "        if not year_filtered.empty:\n",
    "            return year_filtered.iloc[0]\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T02:35:44.122132Z",
     "iopub.status.busy": "2025-01-02T02:35:44.121722Z",
     "iopub.status.idle": "2025-01-02T02:35:44.127961Z",
     "shell.execute_reply": "2025-01-02T02:35:44.126855Z",
     "shell.execute_reply.started": "2025-01-02T02:35:44.122100Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def find_first_word_match(title, year, tmdb_df):\n",
    "    \"\"\"\n",
    "    Find matches based on the first word of the title.\n",
    "    \"\"\"\n",
    "    first_word = title.split()[0] if title else ''\n",
    "    if first_word:\n",
    "        matches = tmdb_df[\n",
    "            (tmdb_df['clean_title'].str.startswith(first_word + ' ', na=False)) |\n",
    "            (tmdb_df['clean_original_title'].str.startswith(first_word + ' ', na=False))\n",
    "        ]\n",
    "        if not matches.empty:\n",
    "            year_filtered = matches[matches['release_year'].apply(lambda x: years_match(x, year))]\n",
    "            if not year_filtered.empty:\n",
    "                return year_filtered.iloc[0]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T02:36:02.804339Z",
     "iopub.status.busy": "2025-01-02T02:36:02.803871Z",
     "iopub.status.idle": "2025-01-02T02:36:02.811401Z",
     "shell.execute_reply": "2025-01-02T02:36:02.810329Z",
     "shell.execute_reply.started": "2025-01-02T02:36:02.804301Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def find_bracket_match(title, year, tmdb_df):\n",
    "    \"\"\"\n",
    "    Find matches based on text before or inside brackets.\n",
    "    \"\"\"\n",
    "    if '(' in title:\n",
    "        # Try matching the part before brackets\n",
    "        before_brackets = title.split('(')[0].strip()\n",
    "        matches = tmdb_df[\n",
    "            (tmdb_df['clean_title'] == before_brackets) |\n",
    "            (tmdb_df['clean_original_title'] == before_brackets)\n",
    "        ]\n",
    "        if not matches.empty:\n",
    "            year_filtered = matches[matches['release_year'].apply(lambda x: years_match(x, year))]\n",
    "            if not year_filtered.empty:\n",
    "                return year_filtered.iloc[0]\n",
    "        \n",
    "        # Try matching the part inside brackets\n",
    "        inside_brackets = title[title.find(\"(\")+1:title.find(\")\")].strip()\n",
    "        matches = tmdb_df[\n",
    "            (tmdb_df['clean_title'] == inside_brackets) |\n",
    "            (tmdb_df['clean_original_title'] == inside_brackets)\n",
    "        ]\n",
    "        if not matches.empty:\n",
    "            year_filtered = matches[matches['release_year'].apply(lambda x: years_match(x, year))]\n",
    "            if not year_filtered.empty:\n",
    "                return year_filtered.iloc[0]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T02:36:24.372823Z",
     "iopub.status.busy": "2025-01-02T02:36:24.372490Z",
     "iopub.status.idle": "2025-01-02T02:36:24.378768Z",
     "shell.execute_reply": "2025-01-02T02:36:24.377713Z",
     "shell.execute_reply.started": "2025-01-02T02:36:24.372792Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def find_matching_movie(row, tmdb_df):\n",
    "    \"\"\"\n",
    "    Find a matching movie using various matching strategies.\n",
    "    \"\"\"\n",
    "    if pd.isna(row['clean_title']):\n",
    "        return None\n",
    "        \n",
    "    year = row.get('year', None)\n",
    "    title = row['clean_title']\n",
    "    \n",
    "    # Try different matching strategies in order\n",
    "    match = find_exact_match(title, year, tmdb_df)\n",
    "    if match is not None:\n",
    "        return match\n",
    "    \n",
    "    match = find_first_word_match(title, year, tmdb_df)\n",
    "    if match is not None:\n",
    "        return match\n",
    "    \n",
    "    match = find_bracket_match(title, year, tmdb_df)\n",
    "    if match is not None:\n",
    "        return match\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T03:22:29.431956Z",
     "iopub.status.busy": "2025-01-02T03:22:29.431530Z",
     "iopub.status.idle": "2025-01-02T03:22:29.443659Z",
     "shell.execute_reply": "2025-01-02T03:22:29.442478Z",
     "shell.execute_reply.started": "2025-01-02T03:22:29.431919Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def process_movie_data(original_df, tmdb_df):\n",
    "    \"\"\"\n",
    "    Main function to process and merge movie data.\n",
    "    \"\"\"\n",
    "    # Preprocess dataframes\n",
    "    original_df, tmdb_df = preprocess_dataframes(original_df, tmdb_df)\n",
    "    \n",
    "    # Initialize output dataframes\n",
    "    plots_df = pd.DataFrame(columns=['id', 'plot'])\n",
    "    metadata_df = pd.DataFrame(columns=[\n",
    "        'id', 'title', 'vote_count', 'vote_average', 'runtime',\n",
    "        'adult', 'original_language', 'popularity', 'poster_path',\n",
    "        'backdrop_path', 'genres', 'production_countries',\n",
    "        'release_date', 'spoken_languages', 'keywords'\n",
    "    ])\n",
    "    \n",
    "    metadata_rows = []\n",
    "    plot_rows = []\n",
    "    removed_titles = []\n",
    "    \n",
    "    with tqdm(total=len(original_df), desc=\"Processing Movies\", unit=\"movie\") as progress:\n",
    "        for i in range(len(original_df)):\n",
    "            row = original_df.iloc[i]\n",
    "            try:\n",
    "                progress.set_postfix_str(f\"Currently processing: {row['Title']}\")\n",
    "                \n",
    "                if pd.isna(row['Plot']):\n",
    "                    removed_titles.append(row['Title'])\n",
    "                    progress.update(1)\n",
    "                    continue\n",
    "                \n",
    "                match = find_matching_movie(row, tmdb_df)\n",
    "                \n",
    "                if match is not None:\n",
    "                    clean_plot = re.sub(r'\\[.*?\\]', '', row['Plot'])\n",
    "                    plot_rows.append({\n",
    "                        'id': match['id'],\n",
    "                        'plot': clean_plot.strip()\n",
    "                    })\n",
    "                \n",
    "                    metadata_row = match[metadata_df.columns].copy()\n",
    "                    if pd.isna(metadata_row['production_countries']):\n",
    "                        metadata_row['production_countries'] = row['Country']\n",
    "                    metadata_rows.append(metadata_row)\n",
    "                else:\n",
    "                    removed_titles.append(row['Title'])\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing row: {row['Title'] if 'Title' in row else 'Unknown'}\")\n",
    "                print(f\"Error: {e}\")\n",
    "                removed_titles.append(row['Title'])\n",
    "            finally:\n",
    "                progress.update(1)\n",
    "    \n",
    "    if plot_rows:\n",
    "        plots_df = pd.DataFrame(plot_rows)\n",
    "    if metadata_rows:\n",
    "        metadata_df = pd.DataFrame(metadata_rows)\n",
    "    \n",
    "    print(\"Done merging datasets\")\n",
    "    print(f\"Number of rows processed: {len(original_df)}\")\n",
    "    print(f\"Number of removed titles: {len(removed_titles)}\")\n",
    "    print(\"Removed Titles:\")\n",
    "    for title in removed_titles:\n",
    "        print(f\"- {title}\")\n",
    "    \n",
    "    return plots_df, metadata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T02:37:09.001895Z",
     "iopub.status.busy": "2025-01-02T02:37:09.001503Z",
     "iopub.status.idle": "2025-01-02T02:44:05.535799Z",
     "shell.execute_reply": "2025-01-02T02:44:05.534672Z",
     "shell.execute_reply.started": "2025-01-02T02:37:09.001858Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Movies: 994movie [06:43,  2.46movie/s, Currently processing: A World Apart]                                                                                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done merging datasets\n",
      "Number of rows processed: 956\n",
      "Number of removed titles: 101\n",
      "Removed Titles:\n",
      "- Zinabu\n",
      "- Goldwidows: Women in Lesotho 1991\n",
      "- Bawina\n",
      "- Yogera\n",
      "- Sahara Occidental indépendance ou génocide?\n",
      "- The Epic of Cheikh Bouamama(الشيخ بوعمامة)\n",
      "- Outside the Law(خارجون عن القانون)\n",
      "- L'Oranais(الوهراني)\n",
      "- Delice Paloma\n",
      "- Si-Gueriki, la reine-mère\n",
      "- Wend Kuuni\n",
      "- Ribo ou le soleil sauvage\n",
      "- Enah Johnscott\n",
      "- Léonie Yangba Zowe\n",
      "- Camille Lepage\n",
      "- Andre kolingbe\n",
      "- Rançon d'une alliance, La\n",
      "- Weddad\n",
      "- Rossassa Fel Qalb(Bullet in the Heart)\n",
      "- Berlanti(Berlanti)\n",
      "- Al-Millionairah al-Saghirah(The Small Millionaire)\n",
      "- Akhlaq Lil Bai(Virtue for Sale)\n",
      "- Al-Ustazah Fatimah(Miss Fatimah)\n",
      "- Al-Malak al-Zalem(The Unjust Angel)\n",
      "- Ayyamna al-Holwa(Our Beautiful Days)\n",
      "- Sayyidat al-Qasr(Lady of the Castle)\n",
      "- Bidaya wa Nihaya(Beginning and End)\n",
      "- Lokmet El-Aish(A Scrap of Bread)\n",
      "- Salladin the Victorious(Al Nasser Salah Al Din)\n",
      "- Mirati Modeer Aam(My Wife, the Director General)\n",
      "- Khally Balak Min ZouZou(Watch Out for ZouZou)\n",
      "- Al-Karnak(Karnak)\n",
      "- Shafika we metwally(Shafika and metwally)\n",
      "- Lahzet Da’af(A Moment of Weakness)\n",
      "- Al-halfout(The Vile)\n",
      "- El-Osta El-Modeer(Monsieur le Directeur)\n",
      "- Al-Darga Al-Thalitha(The 3rd Class)\n",
      "- Al-raqissa wa-l-siyasi(The Belly Dancer and the Politician)\n",
      "- Al-Kit Kat\n",
      "- Losoos Khamas Nogoom(Five-Star Thieves)\n",
      "- Selanchi\n",
      "- Invités surprises\n",
      "- Fundi-Mentals\n",
      "- Saïkati\n",
      "- Shuga\n",
      "- Kennis Voor Het Leven\n",
      "- The Wind\n",
      "- Watani, un monde sans mal\n",
      "- Alyam, Alyam\n",
      "- Alyam Alyam O les jours aka Oh the days\n",
      "- DP75: Tartina City\n",
      "- Queen of the Desert\n",
      "- citation needed\n",
      "- citation needed\n",
      "- Solveig Nordlund\n",
      "- citation needed\n",
      "- citation needed\n",
      "- citation needed\n",
      "- citation needed\n",
      "- Marrabenta Stories\n",
      "- citation needed\n",
      "- citation needed\n",
      "- citation needed\n",
      "- citation needed\n",
      "- Koukan Kourcia(Le cri de la tourterelle)\n",
      "- Akounak Tedalat Taha Tazoughai\n",
      "- Zin'naariya\n",
      "- Glamour Girls\n",
      "- Owo Blow\n",
      "- Karishika 2\n",
      "- Issakaba1-4\n",
      "- Jenifa1 and 2\n",
      "- Holding Hope\n",
      "- Inale\n",
      "- Fuelling Poverty\n",
      "- Hoodrush\n",
      "- Kamara's Tree\n",
      "- Suru L'ere\n",
      "- Ehi's Bitters\n",
      "- Udoka Onyeka\n",
      "- Things Fall Apart\n",
      "- Efunsetan Aniwura\n",
      "- Things Fall Apart\n",
      "- Sarie Marais\n",
      "- Sarie Marais\n",
      "- Vreemde Wêreld\n",
      "- You're in the Movies\n",
      "- Mountain of Hell\n",
      "- Lucky Strikes Back\n",
      "- Have You Seen Drum Recently?\n",
      "- Son of Man\n",
      "- Jerusalema\n",
      "- Dora's Peace\n",
      "- Sink\n",
      "- Taking Earth\n",
      "- Parable\n",
      "- Anneaux d'or\n",
      "- Safa'ih min dhahab\n",
      "- Toefl Al-Shams\n",
      "- Kushata Kwemoyo\n",
      "- Thandie's Diary\n"
     ]
    }
   ],
   "source": [
    "plots_df, metadata_df = process_movie_data(african_df, tmdb_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T02:44:28.111543Z",
     "iopub.status.busy": "2025-01-02T02:44:28.111036Z",
     "iopub.status.idle": "2025-01-02T02:44:28.118802Z",
     "shell.execute_reply": "2025-01-02T02:44:28.117752Z",
     "shell.execute_reply.started": "2025-01-02T02:44:28.111503Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(855, 2)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plots_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T02:44:56.202087Z",
     "iopub.status.busy": "2025-01-02T02:44:56.201626Z",
     "iopub.status.idle": "2025-01-02T02:44:56.243134Z",
     "shell.execute_reply": "2025-01-02T02:44:56.242092Z",
     "shell.execute_reply.started": "2025-01-02T02:44:56.202047Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plots_df.to_csv('african_movie_plots.csv', index=False)\n",
    "metadata_df.to_csv('african_movie_metadata.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6409907,
     "sourceId": 10351299,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6410086,
     "sourceId": 10351668,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
